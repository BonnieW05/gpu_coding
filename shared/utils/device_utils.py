"""
è®¾å¤‡å·¥å…·å‡½æ•°
ç”¨äºGPUè®¾å¤‡ç®¡ç†å’Œä¿¡æ¯è·å–
"""

import torch
import subprocess
import platform

def get_gpu_info():
    """è·å–GPUä¿¡æ¯"""
    if not torch.cuda.is_available():
        return {"available": False, "message": "CUDAä¸å¯ç”¨"}
    
    gpu_info = {
        "available": True,
        "device_count": torch.cuda.device_count(),
        "current_device": torch.cuda.current_device(),
        "device_name": torch.cuda.get_device_name(),
        "memory_allocated": torch.cuda.memory_allocated(),
        "memory_reserved": torch.cuda.memory_reserved(),
        "max_memory_allocated": torch.cuda.max_memory_allocated(),
        "max_memory_reserved": torch.cuda.max_memory_reserved(),
    }
    
    return gpu_info

def print_gpu_info():
    """æ‰“å°GPUä¿¡æ¯"""
    info = get_gpu_info()
    
    if not info["available"]:
        print("âŒ CUDAä¸å¯ç”¨")
        return
    
    print("ğŸ–¥ï¸  GPUä¿¡æ¯:")
    print(f"   è®¾å¤‡æ•°é‡: {info['device_count']}")
    print(f"   å½“å‰è®¾å¤‡: {info['current_device']}")
    print(f"   è®¾å¤‡åç§°: {info['device_name']}")
    print(f"   å·²åˆ†é…å†…å­˜: {info['memory_allocated'] / 1024**3:.2f} GB")
    print(f"   å·²ä¿ç•™å†…å­˜: {info['memory_reserved'] / 1024**3:.2f} GB")
    print(f"   æœ€å¤§åˆ†é…å†…å­˜: {info['max_memory_allocated'] / 1024**3:.2f} GB")
    print(f"   æœ€å¤§ä¿ç•™å†…å­˜: {info['max_memory_reserved'] / 1024**3:.2f} GB")

def get_nvidia_smi_info():
    """è·å–nvidia-smiä¿¡æ¯"""
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            return result.stdout
        else:
            return "nvidia-smiå‘½ä»¤æ‰§è¡Œå¤±è´¥"
    except FileNotFoundError:
        return "nvidia-smiå‘½ä»¤æœªæ‰¾åˆ°"

def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("âœ… GPUå†…å­˜å·²æ¸…ç†")

def set_device(device_id=None):
    """è®¾ç½®CUDAè®¾å¤‡"""
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è®¾ç½®è®¾å¤‡")
        return None
    
    if device_id is None:
        device_id = 0
    
    if device_id >= torch.cuda.device_count():
        print(f"âŒ è®¾å¤‡ID {device_id} è¶…å‡ºèŒƒå›´")
        return None
    
    torch.cuda.set_device(device_id)
    print(f"âœ… å·²è®¾ç½®è®¾å¤‡ä¸º: {torch.cuda.get_device_name(device_id)}")
    return device_id

def benchmark_function(func, *args, **kwargs):
    """å‡½æ•°æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    import time
    
    # é¢„çƒ­
    for _ in range(10):
        func(*args, **kwargs)
    
    # åŒæ­¥GPU
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    # è®¡æ—¶
    start_time = time.time()
    for _ in range(100):
        result = func(*args, **kwargs)
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    end_time = time.time()
    avg_time = (end_time - start_time) / 100
    
    print(f"â±ï¸  å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time*1000:.2f} ms")
    return result, avg_time

if __name__ == "__main__":
    print_gpu_info()
    print("\n" + "="*50)
    print(get_nvidia_smi_info())
