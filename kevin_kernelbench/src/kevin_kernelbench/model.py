"""
Kevin-32B æ¨¡å‹æ¥å£
"""

import torch
import logging
import time
from typing import Dict, Any, List
from transformers import AutoModelForCausalLM, AutoTokenizer

logger = logging.getLogger(__name__)

# å¯é€‰å¯¼å…¥swanlab
try:
    import swanlab
    SWANLAB_AVAILABLE = True
except ImportError:
    SWANLAB_AVAILABLE = False
    swanlab = None


class KevinModel:
    """Kevin-32B æ¨¡å‹åŒ…è£…å™¨ï¼Œç”¨äºç”ŸæˆCUDAå†…æ ¸ä»£ç """
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–Kevinæ¨¡å‹"""
        self.config = config
        self.model = None
        self.tokenizer = None
        self.swanlab_run = None
        self._load_model()
        
        # åˆå§‹åŒ–SwanLabï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if SWANLAB_AVAILABLE and config.get("monitoring", {}).get("use_swanlab", False):
            try:
                self.swanlab_run = swanlab.init(
                    project="kevin-kernelbench",
                    experiment_name=f"model_generation_{int(time.time())}",
                    description="Kevin-32B CUDA kernel generation"
                )
                logger.info("SwanLabæ¨¡å‹è·Ÿè¸ªå·²åˆå§‹åŒ–")
            except Exception as e:
                logger.warning(f"SwanLabæ¨¡å‹è·Ÿè¸ªåˆå§‹åŒ–å¤±è´¥: {e}")
                self.swanlab_run = None
    
    def _load_model(self):
        """åŠ è½½Kevin-32Bæ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œé‡‡ç”¨æ¸è¿›å¼åŠ è½½ç­–ç•¥"""
        logger.info("å¼€å§‹åŠ è½½Kevin-32Bæ¨¡å‹...")
        
        # ç­–ç•¥1: å°è¯•å†…å­˜å®‰å…¨åŠ è½½ï¼ˆä»HuggingFace Hubï¼‰
        if self._try_load_from_hub():
            logger.info("âœ… æˆåŠŸä»HuggingFace HubåŠ è½½æ¨¡å‹ï¼ˆå†…å­˜å®‰å…¨æ¨¡å¼ï¼‰")
            return
        
        # ç­–ç•¥2: å°è¯•æœ¬åœ°åŠ è½½ï¼ˆå†…å­˜å®‰å…¨æ¨¡å¼ï¼‰
        if self._try_load_from_local():
            logger.info("âœ… æˆåŠŸä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼ˆå†…å­˜å®‰å…¨æ¨¡å¼ï¼‰")
            return
        
        # ç­–ç•¥3: æ”¾å¼ƒåŠ è½½
        logger.error("âŒ æ‰€æœ‰åŠ è½½ç­–ç•¥éƒ½å¤±è´¥ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
        raise RuntimeError("æ— æ³•åŠ è½½Kevin-32Bæ¨¡å‹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæœ¬åœ°æ–‡ä»¶")
    
    def _try_load_from_hub(self):
        """å°è¯•ä»HuggingFace HubåŠ è½½æ¨¡å‹ï¼ˆå†…å­˜å®‰å…¨æ¨¡å¼ï¼‰"""
        try:
            logger.info("ğŸ”„ å°è¯•ä»HuggingFace HubåŠ è½½æ¨¡å‹...")
            
            # å†…å­˜å®‰å…¨çš„åˆ†è¯å™¨åŠ è½½
            tokenizer_kwargs = {
                "cache_dir": self.config["model"]["cache_dir"],
                "trust_remote_code": True,
                "local_files_only": False  # å…è®¸ä»ç½‘ç»œä¸‹è½½
            }
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config["model"]["name"],
                **tokenizer_kwargs
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # å†…å­˜å®‰å…¨çš„æ¨¡å‹åŠ è½½
            model_kwargs = {
                "cache_dir": self.config["model"]["cache_dir"],
                "torch_dtype": torch.float16,
                "device_map": "auto",
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
                "local_files_only": False  # å…è®¸ä»ç½‘ç»œä¸‹è½½
            }
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config["model"]["name"],
                **model_kwargs
            )
            
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä»HuggingFace HubåŠ è½½å¤±è´¥: {e}")
            self.model = None
            self.tokenizer = None
            return False
    
    def _try_load_from_local(self):
        """å°è¯•ä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼ˆå†…å­˜å®‰å…¨æ¨¡å¼ï¼‰"""
        try:
            logger.info("ğŸ”„ å°è¯•ä»æœ¬åœ°åŠ è½½æ¨¡å‹...")
            
            # æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„
            local_model_path = self.config["model"].get("local_path")
            if not local_model_path:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šæœ¬åœ°è·¯å¾„ï¼Œå°è¯•ä»cache_diråŠ è½½
                local_model_path = self.config["model"]["cache_dir"]
            
            # å†…å­˜å®‰å…¨çš„æœ¬åœ°åˆ†è¯å™¨åŠ è½½
            tokenizer_kwargs = {
                "cache_dir": local_model_path,
                "trust_remote_code": True,
                "local_files_only": True  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            }
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                local_model_path,
                **tokenizer_kwargs
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # å†…å­˜å®‰å…¨çš„æœ¬åœ°æ¨¡å‹åŠ è½½
            model_kwargs = {
                "cache_dir": local_model_path,
                "torch_dtype": torch.float16,
                "device_map": "auto",
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
                "local_files_only": True  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            }
            
            self.model = AutoModelForCausalLM.from_pretrained(
                local_model_path,
                **model_kwargs
            )
            
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä»æœ¬åœ°åŠ è½½å¤±è´¥: {e}")
            self.model = None
            self.tokenizer = None
            return False
    
    def generate_kernel(self, prompt: str) -> str:
        """ç”Ÿæˆå•ä¸ªCUDAå†…æ ¸"""
        start_time = time.time()
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        # å°†è¾“å…¥ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        if hasattr(self.model, 'device'):
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config["generation"]["max_new_tokens"],
                temperature=self.config["generation"]["temperature"],
                top_p=self.config["generation"]["top_p"],
                top_k=self.config["generation"]["top_k"],
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        result = generated_text[len(prompt):].strip()
        
        # è®°å½•åˆ°SwanLab
        if self.swanlab_run:
            try:
                generation_time = time.time() - start_time
                self.swanlab_run.log({
                    "generation_time": generation_time,
                    "prompt_length": len(prompt),
                    "generated_length": len(result),
                    "tokens_per_second": len(result.split()) / generation_time if generation_time > 0 else 0
                })
            except Exception as e:
                logger.warning(f"SwanLabç”Ÿæˆæ—¥å¿—è®°å½•å¤±è´¥: {e}")
        
        return result
    
    def generate_batch_kernels(self, prompts: List[str]) -> List[str]:
        """æ‰¹é‡ç”ŸæˆCUDAå†…æ ¸"""
        results = []
        for prompt in prompts:
            try:
                kernel = self.generate_kernel(prompt)
                results.append(kernel)
            except Exception as e:
                logger.error(f"ç”Ÿæˆå†…æ ¸å¤±è´¥: {e}")
                results.append("")
        return results